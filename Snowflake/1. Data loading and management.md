### Introduction to Snowflake
## Overview and Architecture
Snowflake consists of 3 parts:
 - cloud-oriented
 - data platform
 - SaaS (Software as a Service) product.

All of Snowflake's infrastructure runs in the cloud. Benefits of the cloud include:
 - elasticity
 - scalability
 - high-availability
 - cost-efficiency
 - durability

As a DATA PLATFORM there are different functionalities available:
 1. Data Warehouse:
  - structured and relational data
  - ANSI (American National Standards Institute) standard SQL
  - ACID (Atomicity, Consistency, Isolation, and Durability) compliant transactions
  - Data stored in databases, schemas and tables. 

2. Data lake:
 - scalable storage and compute
 - schema does not need to be defined upfront
 - native processing of semi-structured data formats

3. Data Engineering:
 - COPY INTO & snowpipe
 - separate compute clusters
 - tasks and streams
 - all data encrypted at rest and in transit

4. Data Science:
- remove data storage roadblocks with cetralised storage
- partner ecosystem includes data science tooling like Amazon SageMaker

5. Data Sharing:
- Secure data sharing
- data exchange
- BI with snowflake partner ecosystem tools

6. Data Applications:
- Connectors and Drivers
- UDFs (User Defined Functions) and Stored Pipelines
- External UDFs
- Preview features such as snowpark

As a SOFTWARE AS A SERVICE product:
- no management of hardaware
- transparent updates and patches
- subscription payment
- ease of access
- automatic optimisation


# Snowflake's Architecture
It decouples Storage, compute and management services. 
It consists of 3 layers: 
1. Stoarge layer (S3, GCS)
 - persistent and scalable storage residing in cloud providers blob storage. 
 - when data is added, snowflake re-organises the data into its proprietary compressed, columnar table file format and micro-partitioned.
 - 

2. Compute layer (virtual warehouse) / Query layer
 - the nodes of a virtual warehouse cooperate similarly to shared-nothing compute clusters, making use of local caching
 - virtual warehouses come in different sizes - XS to 6XL

3. Service layer 
 - consists of services that range from authentication to query optimisation. 
 - services include:
  * authentication and access control
  * infrastructure management 
  * transaction management
  * metadata management
  * query parsing and optimisation
  * security

Key Features and Capabilities:

- Scalability: Automatically scales compute resources to meet demand without manual intervention.
- Concurrency and Accessibility: Supports a high number of concurrent users and diverse workloads without performance degradation.
- Performance: Offers fast data processing and analytics capabilities.
- Security: Provides robust security features like encryption, access controls, and compliance with various standards.
- Data Sharing: Facilitates secure and easy sharing of data across different organizations.

## Snowflake data loading and unloading
Data loading methods in snowflake:
- using snowflake's data loading tools
 * snowflake data loading wizzard (GUI)
 * snowSQl (command line tool)
 * snowpipe (continuous data addition service - it triggers a table load whenever new data is detetected in the source)
- bulk loading
 * bulk loading from cloud storage using simple SQl commands
 * bulk loading from on-premesis storage using Snowball Edge integration
- ETL/ ELT tools (Extract / Load / Transform)
 * integrates with ELT / ETL tools like Talend, Informatica, Matillion, Fivetran etc. 
- Others
 * REST API's that enable programs to interact with the data, including loading data into snowflake. 
 * third party connectors

# data loading best practices
 - choose the right file format and use the compression options available.
 - preprocess and stage data files before loading to ensure they are correctly formatted and optimised for bulk loading. 
 - implement clustering keys on tables to optimise data organisation - which can significantly improve query performance in large tables.
 - use COPY INTO command with optimal parameters eg batch size, file format options and parallelism settings.
 - monitor and optimise load performance regularly using snowflake's monitoring tools.
 - optimise load jobs based on historical performance metrics. 

# data unloading best practices
- use parallel unloading by using multiple threads or warehouses. 
- secure unloaded data by applying necessary encryption and access controls
- regularly clean up staging data - remove unnecessary stages data after successful unloading to free up storage resources and maintain a clean environment.

# data transformations
- SQL for data transformation - snowflake supports standard and built in SQL functions as well as window functions like ROW_NUMBER, RANK, LEAD, LAG etc to perform advanced analytical queries and to perform calculations over specific data subsets. 
- views and stored procedures - views provide a logical layer over the underlying data. stored procs encapsulate a series of SQL statements, enabling reusability and modularity in data transformation work flows.
- external functions and UDFs - Javascript UDFs
- snowflake partner ETL / ELT tools - these tools provide drag and drop interfaces and workflows for designing complex data transformations
- data pipelines and snowpipe - design workflows that include transformation steps alongside data insertion or before data loading
- external data transformation - for advanced transformations or specialised processing, data can be extracted, then transformed with tools like Python and then loaded back into snowflake. 

## Snowflake data modelling
data modelling is process of mapping and organising data using simplified diagrams, symbols and text to represent data associations and flows.
data modelling ensures QUALITY and CONSISTENCY of data. 
a schema is a database blueprint.
a data model is an overarching design that determines what can exist in the schema.

A data modeler maps complex software system designs into easy to understand diagrams to represent proper data flows.

*DIFFERENT APPROACHES TO DATA MODELLING:*
- heirarchical - data is organised into tree-like structures with data stores as interconnected records with one-to-many arrangements. (standard in XML and GIS)
- relational - provides methodology for specifying data and queries. (most use SQL)
- entity relationship - use diagrams to portray data and their relationships and integrated with relational data models. 
- graph models are visualisations of complex relationships within data sets that are limited by a chosen domain. 

*types of data models* 
- conceptual
- logical
- physical

# data modelling vs. data architecture
data architecture defines a blueprint for managing data assets by aligning with organisational needs and establish data requirements and designs to meet these requirements.

data modelling and data architecture align when new systems are integrated into an existing system. With data modelling, it is possible to compare data from two systems and integrate smoothly. 


# Snowflake storage model
 - Snowflake follows a multi-cluster, shared data architecture that separates compute resources from storage. Separation allows scaling of both components individually. 
 - Snowflake stores data into micro-partitions, which are compressed, immutable and efficiently organised units of data.
 - tables are stored in columnar format, which aids in faster query performance, especially for analytical workloads. 
 - snowflake automatically compresses data to reduce storage.
 - zero-copy cloning enables multiple tables to share the same data without duplicating storage. 
 - snowflake storage costs are based on the amount of data stored and the duration it is retained.

# Snowflake clustering
it involves physically arranging the data in tables to align with the clustering keys, which helps in minimising I/O (input and output) operations during queries. 
the advantage is improved query performance and lower storage costs.  it reduces the amount of data scanned during queries by storing related data together. `ALTER TABLE table_name CLUSTER BY(column)` Queries that involve filtering or grouping by this column may experience improved performance due to reduced data scanning. 
Clustering keys can be modified to align with different access patterns or query requirements. You can remove clusters or recluster at any point. 
Monitoring Clustering - the Information Schema views allows us to check the clustering status and details of tables.