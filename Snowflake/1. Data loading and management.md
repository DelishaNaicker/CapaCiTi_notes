### Introduction to Snowflake
## Overview and Architecture
Snowflake consists of 3 parts:
 - cloud-oriented
 - data platform
 - SaaS (Software as a Service) product.

All of Snowflake's infrastructure runs in the cloud. Benefits of the cloud include:
 - elasticity
 - scalability
 - high-availability
 - cost-efficiency
 - durability

As a DATA PLATFORM there are different functionalities available:
 1. Data Warehouse:
  - structured and relational data
  - ANSI (American National Standards Institute) standard SQL
  - ACID (Atomicity, Consistency, Isolation, and Durability) compliant transactions
  - Data stored in databases, schemas and tables. 

2. Data lake:
 - scalable storage and compute
 - schema does not need to be defined upfront
 - native processing of semi-structured data formats

3. Data Engineering:
 - COPY INTO & snowpipe
 - separate compute clusters
 - tasks and streams
 - all data encrypted at rest and in transit

4. Data Science:
- remove data storage roadblocks with cetralised storage
- partner ecosystem includes data science tooling like Amazon SageMaker

5. Data Sharing:
- Secure data sharing
- data exchange
- BI with snowflake partner ecosystem tools

6. Data Applications:
- Connectors and Drivers
- UDFs (User Defined Functions) and Stored Pipelines
- External UDFs
- Preview features such as snowpark

As a SOFTWARE AS A SERVICE product:
- no management of hardaware
- transparent updates and patches
- subscription payment
- ease of access
- automatic optimisation


# Snowflake's Architecture
It decouples Storage, compute and management services. 
It consists of 3 layers: 
1. Stoarge layer (S3, GCS)
 - persistent and scalable storage residing in cloud providers blob storage. 
 - when data is added, snowflake re-organises the data into its proprietary compressed, columnar table file format and micro-partitioned.
 - 

2. Compute layer (virtual warehouse) / Query layer
 - the nodes of a virtual warehouse cooperate similarly to shared-nothing compute clusters, making use of local caching
 - virtual warehouses come in different sizes - XS to 6XL

3. Service layer 
 - consists of services that range from authentication to query optimisation. 
 - services include:
  * authentication and access control
  * infrastructure management 
  * transaction management
  * metadata management
  * query parsing and optimisation
  * security

Key Features and Capabilities:

- Scalability: Automatically scales compute resources to meet demand without manual intervention.
- Concurrency and Accessibility: Supports a high number of concurrent users and diverse workloads without performance degradation.
- Performance: Offers fast data processing and analytics capabilities.
- Security: Provides robust security features like encryption, access controls, and compliance with various standards.
- Data Sharing: Facilitates secure and easy sharing of data across different organizations.

## Snowflake data loading and unloading
Data loading methods in snowflake:
- using snowflake's data loading tools
 * snowflake data loading wizzard (GUI)
 * snowSQl (command line tool)
 * snowpipe (continuous data addition service - it triggers a table load whenever new data is detetected in the source)
- bulk loading
 * bulk loading from cloud storage using simple SQl commands
 * bulk loading from on-premesis storage using Snowball Edge integration
- ETL/ ELT tools (Extract / Load / Transform)
 * integrates with ELT / ETL tools like Talend, Informatica, Matillion, Fivetran etc. 
- Others
 * REST API's that enable programs to interact with the data, including loading data into snowflake. 
 * third party connectors

# data loading best practices
 - choose the right file format and use the compression options available.
 - preprocess and stage data files before loading to ensure they are correctly formatted and optimised for bulk loading. 
 - implement clustering keys on tables to optimise data organisation - which can significantly improve query performance in large tables.
 - use COPY INTO command with optimal parameters eg batch size, file format options and parallelism settings.
 - monitor and optimise load performance regularly using snowflake's monitoring tools.
 - optimise load jobs based on historical performance metrics. 

# data unloading best practices
- use parallel unloading by using multiple threads or warehouses. 
- secure unloaded data by applying necessary encryption and access controls
- regularly clean up staging data - remove unnecessary stages data after successful unloading to free up storage resources and maintain a clean environment.

# data transformations
- SQL for data transformation - snowflake supports standard and built in SQL functions as well as window functions like ROW_NUMBER, RANK, LEAD, LAG etc to perform advanced analytical queries and to perform calculations over specific data subsets. 
- views and stored procedures - views provide a logical layer over the underlying data. stored procs encapsulate a series of SQL statements, enabling reusability and modularity in data transformation work flows.
- external functions and UDFs - Javascript UDFs
- snowflake partner ETL / ELT tools - these tools provide drag and drop interfaces and workflows for designing complex data transformations
- data pipelines and snowpipe - design workflows that include transformation steps alongside data insertion or before data loading
- external data transformation - for advanced transformations or specialised processing, data can be extracted, then transformed with tools like Python and then loaded back into snowflake. 



## Snowflake data modelling